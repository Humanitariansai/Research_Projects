{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div style=\"text-align: left\">CSYE 7370 Deep Learning and Reinforcement Learning in Game Engineering</div>\n",
    "<div style=\"text-align: left\">Yucong Feng, 001586064</div>\n",
    "<div style=\"text-align: left\">Final Project</div>\n",
    "\n",
    "## Application of ResNet50 on animal classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**ABSTRACT**\n",
    "\n",
    "The paper aims to use Residual Network 50 (ResNet50) [1] model to classify 12 kinds of animals. The dataset is consisted of different images which are collected from internet. \n",
    "\n",
    "The results show that we can successfully classify these animals based on the animal dataset, which means this can be a useful tool for image recognition in many fields. \n",
    "\n",
    "The image dataset of animals that I used can be found on AIStudio: https://aistudio.baidu.com/aistudio/datasetdetail/68755.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1\tINTRODUCTION\n",
    "With the decline of traditional taxonomy research, there are fewer and fewer taxonomy experts, but there are still a large number of animal and plant specimens and photos that need to be quickly identified and identified; at the same time, field natural education is gradually rising, which needs to be able to quickly identify species and provide relevant tools. Some popular image recognition networks, such as AlexNet, VGGNet, ResNet, and InceptionV1-V4 network, are widely used in image recognition. Since I want our classified image to have higher precision, I chose the ResNet50 as the network I use.\n",
    "\n",
    "Residual Network 50 is a powerful tool in the state of image recognition. Many types of research have been conducted around this topic, and this mode advanced dramatically in recent years. This model was immensely successful, as can be ascertained from the fact that its ensemble won the top position at the ILSVRC 2015 classification competition with an error of only 3.57%. Additionally, it also came first in the ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation in the ILSVRC & COCO competitions of 2015.\n",
    "\n",
    "The dataset of animals is collected from the internet. Figure 1 shows a part of the dataset. This training dataset has a total of 12 different categories of animals, and each category has about 600 different images, a total of 7096 images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2 Data preparation\n",
    "\n",
    "## 2.1 Unzip the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Sample from dataset\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/d279bfc657864d5a910ba98de641f63c7da83a85fd5347de8bf851c3baca3ea0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!unzip -q -o data/data68755/signs.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.2 Data annotation\n",
    "\n",
    "```bash\n",
    ".\n",
    "├── test\n",
    "│   ├── dog\n",
    "│   ├── dragon\n",
    "│   ├── goat\n",
    "│   ├── horse\n",
    "│   ├── monkey\n",
    "│   ├── ox\n",
    "│   ├── pig\n",
    "│   ├── rabbit\n",
    "│   ├── ratt\n",
    "│   ├── rooster\n",
    "│   ├── snake\n",
    "│   └── tiger\n",
    "├── train\n",
    "│   ├── dog\n",
    "│   ├── dragon\n",
    "│   ├── goat\n",
    "│   ├── horse\n",
    "│   ├── monkey\n",
    "│   ├── ox\n",
    "│   ├── pig\n",
    "│   ├── rabbit\n",
    "│   ├── ratt\n",
    "│   ├── rooster\n",
    "│   ├── snake\n",
    "│   └── tiger\n",
    "└── valid\n",
    "    ├── dog\n",
    "    ├── dragon\n",
    "    ├── goat\n",
    "    ├── horse\n",
    "    ├── monkey\n",
    "    ├── ox\n",
    "    ├── pig\n",
    "    ├── rabbit\n",
    "    ├── ratt\n",
    "    ├── rooster\n",
    "    ├── snake\n",
    "    └── tiger\n",
    "```\n",
    "The data set is divided into three folders: train, valid, and test. Each folder contains **12 classification folders**, and each classification folder contains specific sample pictures.\n",
    "\n",
    "We perform an annotation process on these samples, and finally generate three data annotation files:\n",
    "\n",
    "1. **train.txt**, \n",
    "2. **valid.txt**, \n",
    "3. **test.txt**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "from PIL import Image\n",
    "from config import get\n",
    "\n",
    "\n",
    "# Dataset root directory\n",
    "DATA_ROOT = 'signs'\n",
    "\n",
    "# Tag List\n",
    "LABEL_MAP = get('LABEL_MAP')\n",
    "\n",
    "# callout generation function\n",
    "def generate_annotation(mode):\n",
    "    # Create annotation file\n",
    "    with open('{}/{}.txt'.format(DATA_ROOT, mode), 'w') as f:\n",
    "        # Data folders for each purpose: train/valid/test\n",
    "        train_dir = '{}/{}'.format(DATA_ROOT, mode)\n",
    "\n",
    "        # Traversing folders to get the classified folders inside\n",
    "        for path in os.listdir(train_dir):\n",
    "            # The numerical index corresponding to the label, directly use the numerical index when actually labeling\n",
    "            label_index = LABEL_MAP.index(path)\n",
    "\n",
    "            # The path where the image samples are located\n",
    "            image_path = '{}/{}'.format(train_dir, path)\n",
    "\n",
    "            # iterate over all images\n",
    "            for image in os.listdir(image_path):\n",
    "                # Image full path and name\n",
    "                image_file = '{}/{}'.format(image_path, image)\n",
    "                \n",
    "                try:\n",
    "                    # Verify image format\n",
    "                    with open(image_file, 'rb') as f_img:\n",
    "                        image = Image.open(io.BytesIO(f_img.read()))\n",
    "                        image.load()\n",
    "                        \n",
    "                        if image.mode == 'RGB':\n",
    "                            f.write('{}\\t{}\\n'.format(image_file, label_index))\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "\n",
    "generate_annotation('train')  # Generate training set annotation files\n",
    "generate_annotation('valid')  # Generate validation set annotation file\n",
    "generate_annotation('test')   # Generate test set annotation files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.3 Dataset definition\n",
    "\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  def convert_to_list(value, n, name, dtype=np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.0.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import paddle\n",
    "import numpy as np\n",
    "from config import get\n",
    "\n",
    "paddle.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dataset import ZodiacDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Instantiate the dataset class\n",
    "\n",
    "Instantiate the dataset class based on the dataset requirements you are using and see the total sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据集：7096张；验证数据集：639张\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ZodiacDataset(mode='train')\n",
    "valid_dataset = ZodiacDataset(mode='valid')\n",
    "\n",
    "print('train sample：{}images；valid sample：{}images'.format(len(train_dataset), len(valid_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3 Model selection\n",
    "\n",
    "## ResNet50\n",
    "\n",
    "\n",
    "**1）ResNet50 Structure**\n",
    "\n",
    "The original ResNet architecture was ResNet-34, which comprised 34 weighted layers. It provided a novel way to add more convolutional layers to a CNN, without running into the vanishing gradient problem, using the concept of shortcut connections [4]. A shortcut connection “skips over” some layers, converting a regular network to a residual network. \n",
    "\n",
    "The regular network was based on the VGG neural networks (VGG-16 and VGG-19)—each convolutional network had a 3×3 filter. However, a ResNet has fewer filters and is less complex than a VGGNet. A 34-layer ResNet can achieve a performance of 3.6 billion FLOPs, and a smaller 18-layer ResNet can achieve 1.8 billion FLOPs, which is significantly faster than a VGG-19 Network with 19.6 billion FLOPs. Figure 3 shows the difference between these models.\n",
    "\n",
    "The ResNet architecture follows two basic design rules: First, the number of filters in each layer is the same depending on the size of the output feature map. Second, if the feature map’s size is halved, it has double the number of filters to maintain the time complexity of each layer. \n",
    "\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/a9045103588d49b09b35855d347f683af9d1926910ad4b639ccf1ec5c36fb7be)\n",
    "\n",
    "ResNet-50 has an architecture based on the model depicted above, but with one important difference. The 50-layer ResNet uses a bottleneck design for the building block. A bottleneck residual block uses 1×1 convolutions, known as a “bottleneck”, which reduces the number of parameters and matrix multiplications. This enables much faster training of each layer. It uses a stack of three layers rather than two layers. \n",
    "The 50-layer ResNet architecture includes the following elements, as shown in the figure 5 below: \n",
    "\n",
    "1. A 7×7 kernel convolution alongside 64 other kernels with a 2-sized stride; \n",
    "2. A max pooling layer with a 2-sized stride; 9 more layers—3×3,64 kernel convolution, another with 1×1,64 kernels, and a third with 1×1,256 kernels. These 3 layers are repeated 3 times; \n",
    "3. 12 more layers with 1×1,128 kernels, 3×3,128 kernels, and 1×1,512 kernels, iterated 4 times; \n",
    "4. 18 more layers with 1×1,256 cores, and 2 cores 3×3,256 and 1×1,1024, iterated 6 times; \n",
    "5. 9 more layers with 1×1,512 cores, 3×3,512 cores, and 1×1,2048 cores iterated 3 times.  \n",
    "\n",
    "**2）Residual blocks**\n",
    "\n",
    "Residual blocks are the essential building blocks of ResNet networks. To make very deep convolutional structures possible, ResNet adds intermediate inputs to the output of a group of convolution blocks. This is also called skip connections, identity mapping, and “residual connections.\n",
    "The objective of skip connections is to allow smoother gradient flow, and ensure important features carry until the final layers. They do not add computational load to the network.\n",
    "The following diagram illustrates a residual block, where x is the input to the ResNet block—output from previous layers, and F(x) is a small neural network with several convolution blocks\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/b2d06daceb7043a8962ba0bd339129731c1c974233ac4baebb074b912f45f80f)\n",
    "\n",
    "Figure 6 illustrates a residual block, where:\n",
    "1.\tx is the input to the ResNet block—output from previous layers\n",
    "2.\tF(x) is a small neural network with several convolution blocks\n",
    "\n",
    "In mathematical terms, it would mean y= x + F(x) where y is the final output of the layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1217 16:55:51.550292    98 device_context.cc:362] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1\n",
      "W1217 16:55:51.553879    98 device_context.cc:372] device: 0, cuDNN Version: 7.6.\n",
      "2022-12-17 16:55:54,749 - INFO - unique_endpoints {''}\n",
      "2022-12-17 16:55:54,751 - INFO - Downloading resnet50.pdparams from https://paddle-hapi.bj.bcebos.com/models/resnet50.pdparams\n",
      "100%|██████████| 151272/151272 [00:02<00:00, 69711.45it/s]\n",
      "2022-12-17 16:55:57,055 - INFO - File /home/aistudio/.cache/paddle/hapi/weights/resnet50.pdparams md5 checking...\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1303: UserWarning: Skip loading for fc.weight. fc.weight receives a shape [2048, 1000], but the expected shape is [2048, 12].\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1303: UserWarning: Skip loading for fc.bias. fc.bias receives a shape [1000], but the expected shape is [12].\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n"
     ]
    }
   ],
   "source": [
    "network = paddle.vision.models.resnet50(num_classes=get('num_classes'), pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "   Layer (type)         Input Shape          Output Shape         Param #    \n",
      "===============================================================================\n",
      "     Conv2D-1        [[1, 3, 224, 224]]   [1, 64, 112, 112]        9,408     \n",
      "   BatchNorm2D-1    [[1, 64, 112, 112]]   [1, 64, 112, 112]         256      \n",
      "      ReLU-1        [[1, 64, 112, 112]]   [1, 64, 112, 112]          0       \n",
      "    MaxPool2D-1     [[1, 64, 112, 112]]    [1, 64, 56, 56]           0       \n",
      "     Conv2D-3        [[1, 64, 56, 56]]     [1, 64, 56, 56]         4,096     \n",
      "   BatchNorm2D-3     [[1, 64, 56, 56]]     [1, 64, 56, 56]          256      \n",
      "      ReLU-2         [[1, 256, 56, 56]]    [1, 256, 56, 56]          0       \n",
      "     Conv2D-4        [[1, 64, 56, 56]]     [1, 64, 56, 56]        36,864     \n",
      "   BatchNorm2D-4     [[1, 64, 56, 56]]     [1, 64, 56, 56]          256      \n",
      "     Conv2D-5        [[1, 64, 56, 56]]     [1, 256, 56, 56]       16,384     \n",
      "   BatchNorm2D-5     [[1, 256, 56, 56]]    [1, 256, 56, 56]        1,024     \n",
      "     Conv2D-2        [[1, 64, 56, 56]]     [1, 256, 56, 56]       16,384     \n",
      "   BatchNorm2D-2     [[1, 256, 56, 56]]    [1, 256, 56, 56]        1,024     \n",
      " BottleneckBlock-1   [[1, 64, 56, 56]]     [1, 256, 56, 56]          0       \n",
      "     Conv2D-6        [[1, 256, 56, 56]]    [1, 64, 56, 56]        16,384     \n",
      "   BatchNorm2D-6     [[1, 64, 56, 56]]     [1, 64, 56, 56]          256      \n",
      "      ReLU-3         [[1, 256, 56, 56]]    [1, 256, 56, 56]          0       \n",
      "     Conv2D-7        [[1, 64, 56, 56]]     [1, 64, 56, 56]        36,864     \n",
      "   BatchNorm2D-7     [[1, 64, 56, 56]]     [1, 64, 56, 56]          256      \n",
      "     Conv2D-8        [[1, 64, 56, 56]]     [1, 256, 56, 56]       16,384     \n",
      "   BatchNorm2D-8     [[1, 256, 56, 56]]    [1, 256, 56, 56]        1,024     \n",
      " BottleneckBlock-2   [[1, 256, 56, 56]]    [1, 256, 56, 56]          0       \n",
      "     Conv2D-9        [[1, 256, 56, 56]]    [1, 64, 56, 56]        16,384     \n",
      "   BatchNorm2D-9     [[1, 64, 56, 56]]     [1, 64, 56, 56]          256      \n",
      "      ReLU-4         [[1, 256, 56, 56]]    [1, 256, 56, 56]          0       \n",
      "     Conv2D-10       [[1, 64, 56, 56]]     [1, 64, 56, 56]        36,864     \n",
      "  BatchNorm2D-10     [[1, 64, 56, 56]]     [1, 64, 56, 56]          256      \n",
      "     Conv2D-11       [[1, 64, 56, 56]]     [1, 256, 56, 56]       16,384     \n",
      "  BatchNorm2D-11     [[1, 256, 56, 56]]    [1, 256, 56, 56]        1,024     \n",
      " BottleneckBlock-3   [[1, 256, 56, 56]]    [1, 256, 56, 56]          0       \n",
      "     Conv2D-13       [[1, 256, 56, 56]]    [1, 128, 56, 56]       32,768     \n",
      "  BatchNorm2D-13     [[1, 128, 56, 56]]    [1, 128, 56, 56]         512      \n",
      "      ReLU-5         [[1, 512, 28, 28]]    [1, 512, 28, 28]          0       \n",
      "     Conv2D-14       [[1, 128, 56, 56]]    [1, 128, 28, 28]       147,456    \n",
      "  BatchNorm2D-14     [[1, 128, 28, 28]]    [1, 128, 28, 28]         512      \n",
      "     Conv2D-15       [[1, 128, 28, 28]]    [1, 512, 28, 28]       65,536     \n",
      "  BatchNorm2D-15     [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048     \n",
      "     Conv2D-12       [[1, 256, 56, 56]]    [1, 512, 28, 28]       131,072    \n",
      "  BatchNorm2D-12     [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048     \n",
      " BottleneckBlock-4   [[1, 256, 56, 56]]    [1, 512, 28, 28]          0       \n",
      "     Conv2D-16       [[1, 512, 28, 28]]    [1, 128, 28, 28]       65,536     \n",
      "  BatchNorm2D-16     [[1, 128, 28, 28]]    [1, 128, 28, 28]         512      \n",
      "      ReLU-6         [[1, 512, 28, 28]]    [1, 512, 28, 28]          0       \n",
      "     Conv2D-17       [[1, 128, 28, 28]]    [1, 128, 28, 28]       147,456    \n",
      "  BatchNorm2D-17     [[1, 128, 28, 28]]    [1, 128, 28, 28]         512      \n",
      "     Conv2D-18       [[1, 128, 28, 28]]    [1, 512, 28, 28]       65,536     \n",
      "  BatchNorm2D-18     [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048     \n",
      " BottleneckBlock-5   [[1, 512, 28, 28]]    [1, 512, 28, 28]          0       \n",
      "     Conv2D-19       [[1, 512, 28, 28]]    [1, 128, 28, 28]       65,536     \n",
      "  BatchNorm2D-19     [[1, 128, 28, 28]]    [1, 128, 28, 28]         512      \n",
      "      ReLU-7         [[1, 512, 28, 28]]    [1, 512, 28, 28]          0       \n",
      "     Conv2D-20       [[1, 128, 28, 28]]    [1, 128, 28, 28]       147,456    \n",
      "  BatchNorm2D-20     [[1, 128, 28, 28]]    [1, 128, 28, 28]         512      \n",
      "     Conv2D-21       [[1, 128, 28, 28]]    [1, 512, 28, 28]       65,536     \n",
      "  BatchNorm2D-21     [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048     \n",
      " BottleneckBlock-6   [[1, 512, 28, 28]]    [1, 512, 28, 28]          0       \n",
      "     Conv2D-22       [[1, 512, 28, 28]]    [1, 128, 28, 28]       65,536     \n",
      "  BatchNorm2D-22     [[1, 128, 28, 28]]    [1, 128, 28, 28]         512      \n",
      "      ReLU-8         [[1, 512, 28, 28]]    [1, 512, 28, 28]          0       \n",
      "     Conv2D-23       [[1, 128, 28, 28]]    [1, 128, 28, 28]       147,456    \n",
      "  BatchNorm2D-23     [[1, 128, 28, 28]]    [1, 128, 28, 28]         512      \n",
      "     Conv2D-24       [[1, 128, 28, 28]]    [1, 512, 28, 28]       65,536     \n",
      "  BatchNorm2D-24     [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048     \n",
      " BottleneckBlock-7   [[1, 512, 28, 28]]    [1, 512, 28, 28]          0       \n",
      "     Conv2D-26       [[1, 512, 28, 28]]    [1, 256, 28, 28]       131,072    \n",
      "  BatchNorm2D-26     [[1, 256, 28, 28]]    [1, 256, 28, 28]        1,024     \n",
      "      ReLU-9        [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "     Conv2D-27       [[1, 256, 28, 28]]    [1, 256, 14, 14]       589,824    \n",
      "  BatchNorm2D-27     [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \n",
      "     Conv2D-28       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144    \n",
      "  BatchNorm2D-28    [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096     \n",
      "     Conv2D-25       [[1, 512, 28, 28]]   [1, 1024, 14, 14]       524,288    \n",
      "  BatchNorm2D-25    [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096     \n",
      " BottleneckBlock-8   [[1, 512, 28, 28]]   [1, 1024, 14, 14]          0       \n",
      "     Conv2D-29      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144    \n",
      "  BatchNorm2D-29     [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \n",
      "      ReLU-10       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "     Conv2D-30       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824    \n",
      "  BatchNorm2D-30     [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \n",
      "     Conv2D-31       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144    \n",
      "  BatchNorm2D-31    [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096     \n",
      " BottleneckBlock-9  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "     Conv2D-32      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144    \n",
      "  BatchNorm2D-32     [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \n",
      "      ReLU-11       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "     Conv2D-33       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824    \n",
      "  BatchNorm2D-33     [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \n",
      "     Conv2D-34       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144    \n",
      "  BatchNorm2D-34    [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096     \n",
      "BottleneckBlock-10  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "     Conv2D-35      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144    \n",
      "  BatchNorm2D-35     [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \n",
      "      ReLU-12       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "     Conv2D-36       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824    \n",
      "  BatchNorm2D-36     [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \n",
      "     Conv2D-37       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144    \n",
      "  BatchNorm2D-37    [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096     \n",
      "BottleneckBlock-11  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "     Conv2D-38      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144    \n",
      "  BatchNorm2D-38     [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \n",
      "      ReLU-13       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "     Conv2D-39       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824    \n",
      "  BatchNorm2D-39     [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \n",
      "     Conv2D-40       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144    \n",
      "  BatchNorm2D-40    [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096     \n",
      "BottleneckBlock-12  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "     Conv2D-41      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144    \n",
      "  BatchNorm2D-41     [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \n",
      "      ReLU-14       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "     Conv2D-42       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824    \n",
      "  BatchNorm2D-42     [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \n",
      "     Conv2D-43       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144    \n",
      "  BatchNorm2D-43    [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096     \n",
      "BottleneckBlock-13  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "     Conv2D-45      [[1, 1024, 14, 14]]    [1, 512, 14, 14]       524,288    \n",
      "  BatchNorm2D-45     [[1, 512, 14, 14]]    [1, 512, 14, 14]        2,048     \n",
      "      ReLU-15        [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0       \n",
      "     Conv2D-46       [[1, 512, 14, 14]]     [1, 512, 7, 7]       2,359,296   \n",
      "  BatchNorm2D-46      [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048     \n",
      "     Conv2D-47        [[1, 512, 7, 7]]     [1, 2048, 7, 7]       1,048,576   \n",
      "  BatchNorm2D-47     [[1, 2048, 7, 7]]     [1, 2048, 7, 7]         8,192     \n",
      "     Conv2D-44      [[1, 1024, 14, 14]]    [1, 2048, 7, 7]       2,097,152   \n",
      "  BatchNorm2D-44     [[1, 2048, 7, 7]]     [1, 2048, 7, 7]         8,192     \n",
      "BottleneckBlock-14  [[1, 1024, 14, 14]]    [1, 2048, 7, 7]           0       \n",
      "     Conv2D-48       [[1, 2048, 7, 7]]      [1, 512, 7, 7]       1,048,576   \n",
      "  BatchNorm2D-48      [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048     \n",
      "      ReLU-16        [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0       \n",
      "     Conv2D-49        [[1, 512, 7, 7]]      [1, 512, 7, 7]       2,359,296   \n",
      "  BatchNorm2D-49      [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048     \n",
      "     Conv2D-50        [[1, 512, 7, 7]]     [1, 2048, 7, 7]       1,048,576   \n",
      "  BatchNorm2D-50     [[1, 2048, 7, 7]]     [1, 2048, 7, 7]         8,192     \n",
      "BottleneckBlock-15   [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0       \n",
      "     Conv2D-51       [[1, 2048, 7, 7]]      [1, 512, 7, 7]       1,048,576   \n",
      "  BatchNorm2D-51      [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048     \n",
      "      ReLU-17        [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0       \n",
      "     Conv2D-52        [[1, 512, 7, 7]]      [1, 512, 7, 7]       2,359,296   \n",
      "  BatchNorm2D-52      [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048     \n",
      "     Conv2D-53        [[1, 512, 7, 7]]     [1, 2048, 7, 7]       1,048,576   \n",
      "  BatchNorm2D-53     [[1, 2048, 7, 7]]     [1, 2048, 7, 7]         8,192     \n",
      "BottleneckBlock-16   [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0       \n",
      "AdaptiveAvgPool2D-1  [[1, 2048, 7, 7]]     [1, 2048, 1, 1]           0       \n",
      "     Linear-1           [[1, 2048]]            [1, 12]            24,588     \n",
      "===============================================================================\n",
      "Total params: 23,585,740\n",
      "Trainable params: 23,479,500\n",
      "Non-trainable params: 106,240\n",
      "-------------------------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 261.48\n",
      "Params size (MB): 89.97\n",
      "Estimated Total Size (MB): 352.02\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_params': 23585740, 'trainable_params': 23479500}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = paddle.Model(network)\n",
    "model.summary((-1, ) + tuple(get('image_shape')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4 Model Training and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "I use cosine annealing as the method of model optimization. \n",
    "\n",
    "**Cosine annealing** is a type of learning rate schedule that has the effect of starting with a large learning rate that is relatively rapidly decreased to a minimum value before being increased rapidly again. The resetting of the learning rate acts like a simulated restart of the learning process and the re-use of good weights as the starting point of the restart is referred to as a ***\"warm restart\"*** in contrast to a ***\"cold restart\"*** where a new set of small random numbers may be used as a starting point. \n",
    "\n",
    "Set the learning rate of each parameter group using a cosine annealing schedule, where ηmax is set to the initial lr and Tcur is the number of epochs since the last restart in SGDR:\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/1b23cc9a33724c188d2ba8e20a527d20c0ac12082593423087adaec4df361ab9)\n",
    "\n",
    "When last_epoch=-1, sets initial lr as lr. Notice that because the schedule is defined recursively, the learning rate can be simultaneously modified outside this scheduler by other operators. If the learning rate is set solely by this scheduler, the learning rate at each step becomes:\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/7373ef89a04a485d9ba3d677b7e4fb58a61db995bfd54b1ea9d80fc5e3ac0a15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/tensor/creation.py:143: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if data.dtype == np.object:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous step.\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py:89: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if isinstance(slot[0], (np.ndarray, np.bool, numbers.Number)):\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  return (isinstance(seq, collections.Sequence) and\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:648: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 222/222 [==============================] - loss: 0.6253 - acc_top1: 0.7803 - acc_top5: 0.9529 - 701ms/step         \n",
      "save checkpoint at /home/aistudio/chk_points/0\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 0.5096 - acc_top1: 0.9061 - acc_top5: 0.9953 - 795ms/step         \n",
      "Eval samples: 639\n",
      "Epoch 2/20\n",
      "step 222/222 [==============================] - loss: 0.7458 - acc_top1: 0.8512 - acc_top5: 0.9790 - 685ms/step         \n",
      "save checkpoint at /home/aistudio/chk_points/1\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 0.3032 - acc_top1: 0.9327 - acc_top5: 0.9953 - 778ms/step         \n",
      "Eval samples: 639\n",
      "Epoch 3/20\n",
      "step 222/222 [==============================] - loss: 0.8525 - acc_top1: 0.8756 - acc_top5: 0.9831 - 690ms/step         \n",
      "save checkpoint at /home/aistudio/chk_points/2\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 0.0738 - acc_top1: 0.9124 - acc_top5: 0.9953 - 768ms/step         \n",
      "Eval samples: 639\n",
      "Epoch 4/20\n",
      "step 222/222 [==============================] - loss: 0.4492 - acc_top1: 0.8795 - acc_top5: 0.9855 - 687ms/step         \n",
      "save checkpoint at /home/aistudio/chk_points/3\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 0.4024 - acc_top1: 0.9202 - acc_top5: 0.9953 - 770ms/step         \n",
      "Eval samples: 639\n",
      "Epoch 5/20\n",
      "step 222/222 [==============================] - loss: 0.4736 - acc_top1: 0.8902 - acc_top5: 0.9859 - 692ms/step         \n",
      "save checkpoint at /home/aistudio/chk_points/4\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 0.2479 - acc_top1: 0.9233 - acc_top5: 0.9969 - 764ms/step         \n",
      "Eval samples: 639\n",
      "Epoch 6/20\n",
      "step 222/222 [==============================] - loss: 0.2368 - acc_top1: 0.9070 - acc_top5: 0.9886 - 679ms/step         \n",
      "save checkpoint at /home/aistudio/chk_points/5\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 0.1264 - acc_top1: 0.9171 - acc_top5: 0.9953 - 777ms/step         \n",
      "Eval samples: 639\n",
      "Epoch 7/20\n",
      "step 222/222 [==============================] - loss: 0.2321 - acc_top1: 0.9097 - acc_top5: 0.9896 - 680ms/step         \n",
      "save checkpoint at /home/aistudio/chk_points/6\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 0.1595 - acc_top1: 0.9249 - acc_top5: 0.9937 - 794ms/step         \n",
      "Eval samples: 639\n",
      "Epoch 8/20\n",
      "step 222/222 [==============================] - loss: 0.1092 - acc_top1: 0.9212 - acc_top5: 0.9917 - 680ms/step         \n",
      "save checkpoint at /home/aistudio/chk_points/7\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 0.2406 - acc_top1: 0.9390 - acc_top5: 0.9937 - 764ms/step         \n",
      "Eval samples: 639\n",
      "Epoch 9/20\n",
      "step 222/222 [==============================] - loss: 0.4159 - acc_top1: 0.9278 - acc_top5: 0.9921 - 687ms/step         \n",
      "save checkpoint at /home/aistudio/chk_points/8\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 0.0413 - acc_top1: 0.9452 - acc_top5: 0.9984 - 759ms/step         \n",
      "Eval samples: 639\n",
      "Epoch 10/20\n",
      "step 222/222 [==============================] - loss: 0.2438 - acc_top1: 0.9356 - acc_top5: 0.9934 - 696ms/step         \n",
      "save checkpoint at /home/aistudio/chk_points/9\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 0.0958 - acc_top1: 0.9562 - acc_top5: 0.9969 - 777ms/step         \n",
      "Eval samples: 639\n",
      "Epoch 11/20\n",
      "step 222/222 [==============================] - loss: 0.2385 - acc_top1: 0.9394 - acc_top5: 0.9935 - 687ms/step         \n",
      "save checkpoint at /home/aistudio/chk_points/10\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 0.2395 - acc_top1: 0.9358 - acc_top5: 0.9953 - 762ms/step         \n",
      "Eval samples: 639\n",
      "Epoch 12/20\n",
      "step 222/222 [==============================] - loss: 0.1854 - acc_top1: 0.9457 - acc_top5: 0.9949 - 681ms/step         \n",
      "save checkpoint at /home/aistudio/chk_points/11\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 0.1130 - acc_top1: 0.9531 - acc_top5: 0.9969 - 772ms/step         \n",
      "Eval samples: 639\n",
      "Epoch 13/20\n",
      "step 222/222 [==============================] - loss: 0.0868 - acc_top1: 0.9526 - acc_top5: 0.9958 - 685ms/step         \n",
      "save checkpoint at /home/aistudio/chk_points/12\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 0.0748 - acc_top1: 0.9546 - acc_top5: 0.9953 - 780ms/step         \n",
      "Eval samples: 639\n",
      "Epoch 14/20\n",
      "step 222/222 [==============================] - loss: 0.0498 - acc_top1: 0.9560 - acc_top5: 0.9959 - 690ms/step         \n",
      "save checkpoint at /home/aistudio/chk_points/13\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 0.0048 - acc_top1: 0.9593 - acc_top5: 0.9984 - 775ms/step         \n",
      "Eval samples: 639\n",
      "Epoch 15/20\n",
      "step 222/222 [==============================] - loss: 0.1957 - acc_top1: 0.9589 - acc_top5: 0.9959 - 685ms/step         \n",
      "save checkpoint at /home/aistudio/chk_points/14\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 0.0446 - acc_top1: 0.9562 - acc_top5: 1.0000 - 789ms/step         \n",
      "Eval samples: 639\n",
      "Epoch 16/20\n",
      "step 222/222 [==============================] - loss: 0.1588 - acc_top1: 0.9562 - acc_top5: 0.9946 - 680ms/step         \n",
      "save checkpoint at /home/aistudio/chk_points/15\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 0.1122 - acc_top1: 0.9515 - acc_top5: 0.9984 - 777ms/step         \n",
      "Eval samples: 639\n",
      "Epoch 17/20\n",
      "step 222/222 [==============================] - loss: 0.0934 - acc_top1: 0.9594 - acc_top5: 0.9953 - 696ms/step         \n",
      "save checkpoint at /home/aistudio/chk_points/16\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 0.0768 - acc_top1: 0.9671 - acc_top5: 0.9984 - 792ms/step         \n",
      "Eval samples: 639\n",
      "Epoch 18/20\n",
      "step 222/222 [==============================] - loss: 0.3811 - acc_top1: 0.9590 - acc_top5: 0.9961 - 693ms/step         \n",
      "save checkpoint at /home/aistudio/chk_points/17\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 0.0222 - acc_top1: 0.9546 - acc_top5: 1.0000 - 774ms/step         \n",
      "Eval samples: 639\n",
      "Epoch 19/20\n",
      "step 222/222 [==============================] - loss: 0.1974 - acc_top1: 0.9579 - acc_top5: 0.9949 - 678ms/step         \n",
      "save checkpoint at /home/aistudio/chk_points/18\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 0.1715 - acc_top1: 0.9546 - acc_top5: 1.0000 - 761ms/step         \n",
      "Eval samples: 639\n",
      "Epoch 20/20\n",
      "step 222/222 [==============================] - loss: 0.0494 - acc_top1: 0.9569 - acc_top5: 0.9962 - 680ms/step         \n",
      "save checkpoint at /home/aistudio/chk_points/19\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 0.1721 - acc_top1: 0.9499 - acc_top5: 0.9984 - 765ms/step         \n",
      "Eval samples: 639\n",
      "save checkpoint at /home/aistudio/chk_points/final\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = get('epochs')\n",
    "BATCH_SIZE = get('batch_size')\n",
    "\n",
    "def create_optim(parameters):\n",
    "    step_each_epoch = get('total_images') // get('batch_size')\n",
    "    lr = paddle.optimizer.lr.CosineAnnealingDecay(learning_rate=get('LEARNING_RATE.params.lr'),\n",
    "                                                  T_max=step_each_epoch * EPOCHS)\n",
    "\n",
    "    return paddle.optimizer.Momentum(learning_rate=lr,\n",
    "                                     parameters=parameters,\n",
    "                                     weight_decay=paddle.regularizer.L2Decay(get('OPTIMIZER.regularizer.factor')))\n",
    "\n",
    "\n",
    "# Model training configuration\n",
    "model.prepare(create_optim(network.parameters()),  # optimizer\n",
    "              paddle.nn.CrossEntropyLoss(),        # loss function\n",
    "              paddle.metric.Accuracy(topk=(1, 5))) # evaluation Index\n",
    "\n",
    "# The callback function of the training visualization VisualDL tool\n",
    "visualdl = paddle.callbacks.VisualDL(log_dir='visualdl_log')\n",
    "\n",
    "# Start the full process training of the model\n",
    "model.fit(train_dataset,            # training dataset\n",
    "          valid_dataset,            # evaluation dataset\n",
    "          epochs=EPOCHS,            # total training rounds\n",
    "          batch_size=BATCH_SIZE,    # Sample Size Size for Batch Calculation\n",
    "          shuffle=True,             # Whether to shuffle the sample set\n",
    "          verbose=1,                # Log display format\n",
    "          save_dir='./chk_points/', # Staged training model storage path\n",
    "          callbacks=[visualdl])     # The callback function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Model storage\n",
    "\n",
    "Save the model we trained for subsequent evaluation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save(get('model_save_dir'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 5 Model Evaluation and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5.1 Batch prediction test\n",
    "\n",
    "### 5.1.1 Test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试数据集样本量：646\n"
     ]
    }
   ],
   "source": [
    "predict_dataset = ZodiacDataset(mode='test')\n",
    "print('Test dataset sample size：{}'.format(len(predict_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5.1.2 Execution forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict begin...\n",
      "step 618/646 [===========================>..] - ETA: 0s - 29ms/st"
     ]
    }
   ],
   "source": [
    "from paddle.static import InputSpec\n",
    "\n",
    "# Network structure instantiation\n",
    "network = paddle.vision.models.resnet50(num_classes=get('num_classes'))\n",
    "\n",
    "# Model encapsulation\n",
    "model_2 = paddle.Model(network, inputs=[InputSpec(shape=[-1] + get('image_shape'), dtype='float32', name='image')])\n",
    "\n",
    "# Load the trained model\n",
    "model_2.load(get('model_save_dir'))\n",
    "\n",
    "# Model configuration\n",
    "model_2.prepare()\n",
    "\n",
    "# Execution forecast\n",
    "result = model_2.predict(predict_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### EVALUATION METHOD\n",
    "To evaluate the result, we use an evaluation method: Random Sampling, to see the performance of ResNet50.\n",
    "\n",
    "Random Sampling: In a simple random sample (SRS) of a given size, all subsets of a sampling frame have an equal probability of being selected. Each element of the frame thus has an equal probability of selection: the frame is not subdivided or partitioned. Furthermore, any given pair of elements has the same chance of selection as any other such pair. This minimizes bias and simplifies analysis of results. In particular, the variance between individual results within the sample is a good indicator of variance in the overall population, which makes it relatively easy to estimate the accuracy of results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample mapping\n",
    "LABEL_MAP = get('LABEL_MAP')\n",
    "\n",
    "# Random sample display\n",
    "indexs = [2, 38, 56, 92, 100, 303]\n",
    "\n",
    "for idx in indexs:\n",
    "    predict_label = np.argmax(result[0][idx])\n",
    "    real_label = predict_dataset[idx][1]\n",
    "\n",
    "    print('SampleID：{}, RealLabel：{}, Predict：{}'.format(idx, LABEL_MAP[real_label], LABEL_MAP[predict_label]))"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![image.png](attachment:image.png)\n",
    "\n",
    "By reviewing the result, it can be believed that ResNet50 can make great classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 6 Model deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_2.save('infer/zodiac', training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# CONCLUSION\n",
    "\n",
    "In this experiment, it demonstrates that ResNet can make great classification. Fortunately, the result shows a good demonstration of accuracy. However, I believe there should be a more detailed way to quantify the result comprehensively, maybe next time I should try to find some other evaluation metrics to measure the performance of this model. For example, we use Manhattan distance to measure the accuracy of K-means or Fréchet inception distance (FID) in generative model.\n",
    "\n",
    "To sum up, residual network or ResNet was a major innovation that has changed the training of deep convolutional neural networks for tasks related to computer vision. While the original Resnet had 34 layers and used 2-layer blocks, other advanced variants such as the Resnet50 made the use of 3-layer bottleneck blocks to ensure improved accuracy and lesser training time. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
